#!/usr/bin/env python
import sys
import os
import json
import yaml
import codecs
import datetime
import time
from bs4 import BeautifulSoup
import twitter as tw
import healthgraph


if not any([x for x in sys.path if "orla-lib" in x]):
    sys.path.append("/".join(sys.path[0].split("/")[:-1] + ["/orla-lib/"]))

from steps import step, run


def healthgraph_APIobject_toJson(self):
    d = dict()
    if self._resource:
        d["resource"] = self._resource

    for k in self._prop_dict:
        if self._prop_dict[k] is not None:
            v = self._prop_dict[k]
            if k == "uri":
                v = v[1].replace("/fitnessActivities/", "/activity/")
            elif k == "duration":
                v = int(v)
            elif isinstance(v, (datetime.datetime, datetime.date)):
                v_null = int(time.mktime(v.timetuple()))
                v = v_null
            d[k] = v
    return json.dumps(d)

# Hack in a toJson method. Code based on the existing __str__ method.
healthgraph.resources.APIobject.toJson = healthgraph_APIobject_toJson


def process_html(data_twitter, html_input_file, html_output_file):
    tweets = []
    print "Processing the HTML file {}".format(html_input_file),
    with codecs.open(os.path.join(data_twitter, html_input_file), encoding="utf8") as f:
        soup = BeautifulSoup(f.read(), "html.parser")
        for dmc in soup.find_all("div", class_=["DirectMessage"]):
            sender_id = None
            sender_screen_name = None
            if "data-message-id" in dmc.attrs:
                dmid = dmc.attrs["data-message-id"]
                anchors = dmc.find_all("a", attrs={"data-user-id": True})
                assert len(anchors) == 1, "{} a for {}".format(len(anchors), dmc.prettify())
                if "data-user-id" in anchors[0].attrs:
                    sender_id = anchors[0].attrs["data-user-id"]
                    sender_screen_name = anchors[0].attrs["href"][1:]
                paragraphs = dmc.find_all("p")
                assert len(paragraphs) <= 1, "{} p for {}".format(len(paragraphs), dmc.prettify())
                if len(paragraphs):
                    text = paragraphs[0].getText()
                    timestamp_span = dmc.find_all("span", class_="_timestamp")
                    assert len(timestamp_span) == 1
                    created_at_long = int(timestamp_span[0].attrs["data-time"])
                    tweet = {
                        "created_at": created_at_long,
                        "id": dmid,
                        "recipient_id": 882443870,
                        "recipient_screen_name": "endafarrell_med",
                        "sender_id": sender_id,
                        "sender_screen_name": sender_screen_name,
                        "text": text
                    }
                    tweets.append(json.dumps(tweet))
    assert tweets
    assert len(tweets)
    with codecs.open(os.path.join(data_twitter, html_output_file), encoding="utf8", mode="wb") as f:
        for tweet in tweets:
            f.write(tweet)
            f.write("\n")
    print "wrote {} tweets/DMs to {}".format(len(tweets), html_output_file)
    return tweets


@step(prereqs=["init"], tags="source")
def twitter(data_twitter, config_twitter, skip_source_twitter):
    """

    :param data_twitter:
    :param config_twitter:
    :param skip_source_twitter:
    :return:
    """

    # there's a problem with twitter in that it will only give you the last 200 DMs. Rubbish!
    # So:
    html_input_file = "taken_from_twitter.com.html"
    html_output_file = "twitter.taken_from_twitter.com.jsons"

    if html_input_file in os.listdir(data_twitter) and not html_output_file in os.listdir(data_twitter):
        process_html(data_twitter, html_input_file, html_output_file)

    # Given rate limits, and that there's only the most recent DMs available, what have we already seen?
    ids = set()
    for j in [j for j in os.listdir(data_twitter) if j.startswith("twitter.") and j.endswith(".jsons")]:
        print "About to read {} file {}".format(data_twitter, j),
        with codecs.open(os.path.join(data_twitter, j), encoding="utf8") as f:
            jids = set([json.loads(line)["id"] for line in f])
            print "extracted another {} tweets/DMs".format(len(jids)),
            ids = ids.union(jids)
            print " and now we have {} tweets/DMS".format(len(ids))
    assert ids
    prior_max_id = max(ids) if ids else 0
    print "There are already {} tweets/DMs with the most recent being {}".format(len(ids), prior_max_id)

    if skip_source_twitter in (False, "False", "false", 0, "No", "no"):
        # It seems we go backwards through the timeline
        new_dms = []
        twitter_error = None
        try:
            # Set up the API for calling
            twitter_config = yaml.safe_load(open(config_twitter).read())
            api = tw.Api(consumer_key=twitter_config["twitter_oauth_consumer_key"],
                         consumer_secret=twitter_config["twitter_oauth_consumer_secret"],
                         access_token_key=twitter_config["twitter_oauth_access_token"],
                         access_token_secret=twitter_config["twitter_oauth_access_token_secret"])
            # These will be the most recent (good, as generally I'll already have something somewhat recent).
            dms = api.GetDirectMessages(#max_id=prior_max_id,
                                        # full_text=True)
                                        count=200)
            if not len(dms):
                print "We seem to already have the most recent based on a prior id of {}".format(prior_max_id)
            else:
                new_ids = set()
                while len(dms):
                    for dm in dms:
                        # print dm.AsJsonString()
                        dm_id = dm.GetId()
                        if not (dm_id in ids or dm_id in new_ids):
                            new_ids.add(dm_id)
                            new_dms.append(dm.AsJsonString())

                    # The max_id for the next call will be the min id for what we've already seen. The API seems to do some
                    # sort of "or equal to".
                    import pprint
                    pprint.pprint(new_ids)
                    max_id = min(new_ids) if new_ids else prior_max_id
                    if prior_max_id == max_id:
                        print "After {} dms, max_id is staying the same at {}: break".format(len(new_dms), max_id)
                        break
                    else:
                        prior_max_id = max_id
                    print "After {} dms, max_id is {}: requesting more".format(len(new_dms), max_id),
                    dms = api.GetDirectMessages(max_id=max_id,
                                                # full_text=True)
                                                count=200)
                    print "and got {}".format(len(dms))

        except tw.error.TwitterError, te:
            twitter_error = te
        finally:
            now = datetime.datetime.strftime(datetime.datetime.utcnow(), "%F.%T")
            lta_filename = "{}/twitter.{}.jsons".format(data_twitter, now)
            if new_dms:
                with codecs.open(lta_filename, mode="wb", encoding="utf8") as f:
                    for dm in new_dms:
                        f.write(dm)
                        f.write('\n')
        if twitter_error:
            raise twitter_error


def relocate_smartpix(data_smartpix, filepath):
    """

    :param data_smartpix:
    :param filepath:
    :return:
    """
    print "relocating {}".format(filepath)
    #new_dir, new_filename = None, None
    with codecs.open(filepath, encoding="ISO-8859-1") as fff:
        soup = BeautifulSoup(fff, "lxml")
        root = soup.find("import")
        if not root:
            # Some non-SmartPix file (eg .DS_Store)
            return

        device = root.find("device")
        if not device:
            # device is the meter. ip is the insulin pump
            device = root.find("ip")
        assert device
        new_filename = datetime.datetime.strftime(
            datetime.datetime.strptime("{} {}".format(device.attrs["dt"],
                                                      device.attrs["tm"]),
                                       "%Y-%m-%d %H:%M"),
            "%Y%m%dT%H%M%S")
        new_dir = "{} {}".format(device.attrs["name"], device.attrs["sn"].strip())
        new_dir = os.path.join(data_smartpix, new_dir)

    # I prefer to close the file before moving it.

    if new_dir and new_filename:
        if not os.path.exists(new_dir):
            os.mkdir(new_dir)
        os.rename(filepath, os.path.join(new_dir, new_filename))


@step(prereqs=["init"], tags="source")
def smartpix(data_smartpix):
    """

    :param data_smartpix:
    :return:
    """
    for f in os.listdir(data_smartpix):
        fp = os.path.join(data_smartpix, f)
        if os.path.isfile(fp):
            relocate_smartpix(data_smartpix, fp)
        else:
            if os.path.isdir(fp) and fp.endswith(".XML"):
                for f2 in os.listdir(fp):
                    relocate_smartpix(data_smartpix, os.path.join(data_smartpix, f, f2))


@step(prereqs=["init"], tags="source")
def runkeeper(data_runkeeper, config_runkeeper):
    """
    
    :param data_runkeeper:
    :param config_runkeeper:
    :return:
    """
    runkeeper_config = yaml.safe_load(open(config_runkeeper).read())

    # TODO - incorporate the functionality of the rk.py file
    access_token = runkeeper_config["PersonalHackAccessToken"]

    user = healthgraph.User(session=healthgraph.Session(access_token))
    profile = user.get_profile()
    act_iter = user.get_fitness_activity_iter()
    activities = list(act_iter)

    now = datetime.datetime.strftime(datetime.datetime.utcnow(), "%F.%T")
    lta_filename = "{}/runkeeper.{}.jsons".format(data_runkeeper, now)
    if activities:
        with codecs.open(lta_filename, mode="wb", encoding="utf8") as f:
            f.write("// ")
            f.write(profile.toJson())
            f.write('\n')
            for a in activities:
                f.write(a.toJson())
                f.write('\n')


if __name__ == "__main__":
    run(locals())
